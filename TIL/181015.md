# kubelet에 GPU resource 사용하도록 하기(NVIDIA GPU 기준)
* `--feature-gates`에서 `Accelerator`항목은 Kubernetes v1.10.x 이후 deprecated됨.
* `Accelerator` 대신 `DevicePlugins`를 추가.
* `/etc/systemd/system/kubelet.service.d/10-kubeadm.conf`에 다음 항목 추가
    * Environment의 KUBELET_KUBECONFIG_ARGS의 맨 끝에 `--feature-gates=\"DevicePlugins=true\"`
* 저장 후
    * `sudo systemctl daemon-reload`와 `sudo systemctl restart kubelet` 

# kubeadm으로 만든 master node `Not Ready`상태 해결하기
* Network overlay 문제로 보임(CNI 문제)
* 다음 추가
    * `kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml`
    * `kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/k8s-manifests/kube-flannel-rbac.yml`

# Local에 master와 slave 같이 돌리기
* kubernetes에서는 모든 작업은 slave(node)에서 이루어지는데, kubelet과 kubeadm 만 세팅해서는 local machine에 slave node를 만들 수 없다.
* Local machine에서 돌리기 위한 방법 중 하나로 minikube를 선택했다.
* minikube는 vm을 이용해 node를 생성한다.
    * 이 node에서 GPU를 사용하려면 vm을 VirtualBox가 아닌 kvm2를 설치해야한다.
    * https://github.com/kubernetes/minikube/blob/master/docs/gpu.md
    * GRUB에서 `GRUB_CMDLINE_LINUX_DEFAULT`에 iommu사용을 추가해야한다. (`etc/default/grub`)
        * Intel의 경우 `intel_iommu=on`
        * AMD의 경우 `amd_iommu=on`
        * 수정 후 `update-grub`과 `reboot`실행
        * 또한 각각의 CPU Architecture에서 Intel: VT-D, AMD: AMD-VI 지원해야함
        * 허용됐는지 확인하려면 다음과 같은 명령 입력
            * `dmesg | grep -e DMAR -e IOMMU`
            * `find /sys/kernel/iommu_groups/ -type l`
    * 이 방법을 이용해 GPU scheduling을 할 때 slave와 master가 같은 노드인 경우, 여분의 GPU가 없거나 현재 PC에서 graphic card driver가 활성화 되어있으면 쓸 수 없다.
    * **minikube가 kubelet config file을 바꿔버리는 경우도 있으므로 주의할것**
    
# Wifi 이용중인 PC와 아닌 PC를 이용해 master/slave 올리기 (GPU Scheduling)
* 둘의 kubernetes 버전이 적어도 앞의 두 자리는 같아야한다.(ex. 1.11.x까지 만족해야함, 1.11.1, 1.11.2인 경우는 되는지 안되는지 확인안해봄)
* kubernetes v1.10.x이상의 경우 둘다 kubelet config에 다음 추가
    * `--feature-gates="DevicePlugins=true"`
    * 추가 후 `systemctl daemon-reload`, `systemctl restart kubelet`필요
* Wifi 이용중인 PC를 master로 쓸 경우
    * 공유기 설정에서 6443 포트를 포트포워딩 해야함. (master가 6443말고 다른 포트를 쓸 경우 해당 포트 포트포워딩)
    * kubeadm init 시 다음 추가
        * `--apiserver-advertise-address=<공유기 IP>`
    * init 후 나온 명령 모두 수행하고 slave에 token 포함된 명령줄 입력
    * master에서 `kubectl get nodes`를 입력하면 해당 slave가 추가되어있는 것을 볼 수 있음
* Wifi 이용중인 PC가 slave인 경우
    * slave쪽에서 사용할 port를 공유기에서 포트포워딩만 해주면 됨.
* (181017 추가) Network overlay를 해결해주는 framework인 flannel이 8472번 port를 사용하는걸 발견했다!
    * Wifi 이용중인 master/slave 노드는 8472번 port를 포트포워딩 해줘야함. (안해도 될 지도..?)

### slave node에서 GPU resource를 제대로 못잡는 문제
```bash
$ kubectl get pods --all-namespaces
```
이 명령어로 kube system에 설치된 flannel이 잘 running중인지 볼 수 있음.
* flannel이 계속 CrashLoopbackOff 상태임 --> 제대로 돌아가지 않는 상태
    * flannel이 제대로 작동하지 않으면 nvidia device plugin을 slave node쪽에 제대로 전달하지 않음
    * 결과적으로 이 문제가 GPU resource를 제대로 detect하지 못하게 하는 원인인 것 같음.
    * [해당 참조(kubeadm 공식문서)](https://kubernetes.io/ko/docs/setup/independent/create-cluster-kubeadm/#tabs-pod-install-4)
    * [해결방법을 찾은 issue](https://github.com/coreos/flannel/issues/728)
* kubeadm init 시 `--pod-network-cidr=10.244.0.0/16` 추가 --> 해결!

### pod log 확인하는 방법 & describe로 상태 알아보기
```bash
kubectl logs <pod name> --v=<log level>
```
* `--v`옵션을 넣으면 자세한 더 자세한 로그를 얻을 수 있음
    * 0-4: debug level
    * 5-10: trace level
* namespace가 kube-system인 경우 pod을 바로 가져올 수 없음
    * `--namespace=kube-system`과 같이 namespace를 특정지으면 가져올 수 있음
    * describe도 마찬가지임
[해당 issue](https://github.com/kubernetes/kubernetes/issues/35054)
